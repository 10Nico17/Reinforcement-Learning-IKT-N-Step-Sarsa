<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>reinforce API documentation</title>
<meta name="description" content="Robotic Arm Learning Module …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>reinforce</code></h1>
</header>
<section id="section-intro">
<p>Robotic Arm Learning Module.</p>
<p>This module contains functions and utilities for implementing and managing the learning process of a robotic arm
using reinforcement learning techniques, specifically the n-step SARSA algorithm. It includes functions for
pausing execution with optional messages, choosing actions based on the epsilon-greedy strategy, performing n-step
SARSA learning on a single robot arm, and managing learning across multiple processes in parallel.</p>
<p>The module also provides functionality for monitoring the learning process using a queue-based communication
system, helpful in a multi-threaded or multi-process environment. Additionally, it includes utilities for
visualizing the learning progress, saving and loading learned values, and stitching learned sections together
for a comprehensive robotic arm movement strategy.</p>
<p>The core of the module revolves around the <code><a title="reinforce.learn" href="#reinforce.learn">learn()</a></code> and <code><a title="reinforce.n_step_sarsa" href="#reinforce.n_step_sarsa">n_step_sarsa()</a></code> functions, which implement the learning
algorithm. The <code><a title="reinforce.learn_parallel" href="#reinforce.learn_parallel">learn_parallel()</a></code> function extends this by managing multiple learning processes in parallel.
Monitoring and visualization are handled by <code><a title="reinforce.monitor_queue" href="#reinforce.monitor_queue">monitor_queue()</a></code> and <code><a title="reinforce.print_sections" href="#reinforce.print_sections">print_sections()</a></code> functions, respectively.</p>
<p>Dependencies:
- numpy
- matplotlib
- sys
- time
- multiprocessing
- threading
- Robot_Arm (a custom module for robotic arm manipulation)</p>
<p>Example Usage:
The module is designed to be used in a sequential manner, starting with initializing learning parameters,
followed by parallel learning with <code><a title="reinforce.learn_parallel" href="#reinforce.learn_parallel">learn_parallel()</a></code>, and finally stitching together the learned sections
for a complete learning strategy.</p>
<p>Note:
This module assumes the existence of a <code>Robot_Arm</code> class within the <code>environment</code> directory, which is
responsible for the actual robotic arm manipulation and learning algorithm implementations. It also uses
multi-threading and multi-processing for parallel execution and monitoring.</p>
<p>Authors: F. M. Sokol, N. M. Hahn, M. Ubbelohde</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Robotic Arm Learning Module.

This module contains functions and utilities for implementing and managing the learning process of a robotic arm
using reinforcement learning techniques, specifically the n-step SARSA algorithm. It includes functions for
pausing execution with optional messages, choosing actions based on the epsilon-greedy strategy, performing n-step
SARSA learning on a single robot arm, and managing learning across multiple processes in parallel.

The module also provides functionality for monitoring the learning process using a queue-based communication
system, helpful in a multi-threaded or multi-process environment. Additionally, it includes utilities for
visualizing the learning progress, saving and loading learned values, and stitching learned sections together
for a comprehensive robotic arm movement strategy.

The core of the module revolves around the `learn` and `n_step_sarsa` functions, which implement the learning
algorithm. The `learn_parallel` function extends this by managing multiple learning processes in parallel.
Monitoring and visualization are handled by `monitor_queue` and `print_sections` functions, respectively.

Dependencies:
- numpy
- matplotlib
- sys
- time
- multiprocessing
- threading
- Robot_Arm (a custom module for robotic arm manipulation)

Example Usage:
The module is designed to be used in a sequential manner, starting with initializing learning parameters,
followed by parallel learning with `learn_parallel`, and finally stitching together the learned sections
for a complete learning strategy.

Note:
This module assumes the existence of a `Robot_Arm` class within the `environment` directory, which is
responsible for the actual robotic arm manipulation and learning algorithm implementations. It also uses
multi-threading and multi-processing for parallel execution and monitoring.

Authors: F. M. Sokol, N. M. Hahn, M. Ubbelohde
&#34;&#34;&#34;

import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.append(&#39;./environment&#39;)
import time
import Robot_Arm as bot
import gc
import multiprocessing
import threading

# suppress scientific notation
np.set_printoptions(suppress=True)

# number of axis used by the robot arm. Either 3 or 6
num_axis = 3

# total number of sections to be learned
num_sections = 32
# section the parallel learning should start in
section_start = 0
# number of sections that should be learned
learn_sections = 32

def debug_pause(string_to_print=None):
    &#34;&#34;&#34;
    Pause the program execution and optionally print a message.

    This function pauses the execution of the program until the user presses Enter.
    It can optionally print a message before pausing.

    Args:
      string_to_print (str, optional): A string message to be printed before pausing. Default is None.

    Returns:
      None
    &#34;&#34;&#34;
    if string_to_print is not None:
        print(string_to_print)
    input(&#34;Press Enter to continue...&#34;)

def get_action_epsilon_greedy(robot, epsilon: float = 0.1, verbosity_level=0) -&gt; int:
    &#34;&#34;&#34;
    Explore or exploit to choose an action for the robot based on epsilon-greedy strategy.

    This function decides whether to explore a new action randomly or exploit the best known action
    from the robot&#39;s Q-values. The choice between exploration and exploitation is made based on the
    epsilon value provided. If exploring, a random action is chosen. If exploiting, the best action
    based on current Q-values is selected.

    Args:
      robot (Six_Axis_Robot_Arm object): The robot on which the action is to be performed.
      epsilon (float): The probability of choosing to explore rather than exploit. Default is 0.1.
      verbosity_level (int): Level of verbosity for output messages. Default is 0.

    Returns:
      tuple: A tuple containing the action in the format of six float values and the action index.
    &#34;&#34;&#34;
    if np.random.uniform(0, 1) &lt; epsilon:
        # Explore: select a random action
        action = robot.get_random_action()
        if verbosity_level &gt;= 2:
            print(f&#34;  Explore with action: {action}&#34;)
        return action
    else:
        # Exploit: select the best action based on current Q-values
        current_qs = robot.get_current_qs()
        action = np.argmax(current_qs)
        action_tuple = (robot.get_action_from_dict(action), action)
        if verbosity_level &gt;= 2:
            print(f&#34;  Exploiting with action: {action_tuple}&#34;)
        return action_tuple

def n_step_sarsa(robot, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1, verbosity_level=0,
                 queue=None, section=None, episode_start=0):
    &#34;&#34;&#34;
    Perform the n-step SARSA algorithm on a given robot for a specified number of episodes.

    This function implements the n-step SARSA learning algorithm. It iteratively updates the Q-values
    based on the observed rewards and the chosen actions using an epsilon-greedy policy. The function
    also allows for dynamic adjustment of the learning rate alpha and supports message passing through
    a queue for parallel execution.

    Args:
      robot (Six_Axis_Robot_Arm object): The robot object to apply the n-step SARSA algorithm on.
      num_episodes (int): The number of episodes to run the algorithm.
      alpha (float): The learning rate. Default is 0.1.
      gamma (float): The discount factor. Default is 0.99.
      epsilon (float): The exploration probability. Default is 0.1.
      verbosity_level (int): Level of verbosity for output messages. Default is 0.
      queue (Queue, optional): A queue object for inter-process communication. Default is None.
      section (any): The section or segment of the task being processed. Default is None.
      episode_start (int): The starting episode number. Default is 0.

    Returns:
      tuple: A tuple containing episode lengths, algorithm name, and the final alpha value.
    &#34;&#34;&#34;
    # n step sarsa, n = 5
    n = 5
    # list to save each episodes length
    episode_lengths = []
    # when queue is given, only write into queue every 4 seconds as to not overwhelm the queue
    # last queue update saves the time of the last update of the queue
    last_queue_update = 0

    # do num_episode episodes
    for episode in range(num_episodes):

        # decrease alpha every 100 episodes by a factor of 0.98 and as long as alpha is &gt; 0.001
        if (alpha &gt; 0.001) and ((episode+episode_start) % 100 == 0) and ((episode+episode_start) != 0):
            alpha *= 0.98

        # time measurement
        start_time = time.time()

        # Initialize the starting state S (choosing from the starting positions)
        robot.reset()

        # debug print
        if verbosity_level &gt;= 2:
            print(f&#34;Robot initialized, starting SARSA, Starting angles: {robot.get_joint_angles_rad()}, Starting position: {robot.rob.end_effector_position()} Choosing action!&#34;)
            print(f&#34;   Initial Qs for starting_voxel:\n{robot.Q[robot.voxels_index_dict[(-500, 0, 0)]]}&#34;)

        # current state contains the current TCP of the robot
        current_state = robot.get_tcp()

        # Choose the first action A from S using Q (epsilon-greedy)
        current_action, current_action_index = get_action_epsilon_greedy(robot, epsilon, verbosity_level=verbosity_level)
        if verbosity_level &gt;= 2:
            print(&#39;current_action: &#39;, current_action)
            print(&#39;current_action_index: &#39;, current_action_index)

        # number of iterations in current episode
        i = 0

        # lists to save last n values
        states = []
        actions = []
        actions_index = []
        rewards = []

        # save first state
        states.append(current_state)
        actions.append(current_action)
        actions_index.append(current_action_index)

        # weightet sum of last rewards
        G = 0

        # initialize best reward for debug printing
        best_reward = -100

        # Send debug information into queue
        if queue is not None:
            if time.time() - last_queue_update &gt; 4:
                queue.put((section, episode, &#34;episode&#34;))
                last_queue_update = time.time()

        # Loop for each step of the episode
        done = False

        # Until all episodes are done
        while not done:

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;\nNew loop, iteration = {i}, current Voxel: {robot.current_voxel}&#34;)
                debug_pause(f&#34; Qs for current Voxel:\n {robot.Q[robot.voxels_index_dict[robot.current_voxel]]}&#34;)

            # Save Q
            last_q = robot.get_current_q(actions_index[0])
            if verbosity_level &gt;= 2:
                print(f&#34;  Q for current action {current_action} with index {current_action_index} at voxel {robot.current_voxel}: {last_q}. Doing SAR&#34;)

            # Take action A, observe R, S&#39;
            new_pos, reward, done = robot.do_move(current_action_index)

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;  New position: {new_pos}, reward: {reward}, new Voxel: {robot.current_voxel}. Doing SA&#34;)
                print(f&#34;  Qs for new position:\n{robot.Q[robot.voxels_index_dict[robot.current_voxel]]}&#34;)

            if reward &gt; best_reward:
                best_reward = reward

            # Choose A&#39; from S&#39; using Q (epsilon-greedy)
            new_action, new_action_index = get_action_epsilon_greedy(robot, epsilon, verbosity_level=verbosity_level)

            # Append states and rewards to lists
            rewards.append(reward)
            states.append(new_pos)
            actions.append(new_action)
            actions_index.append(new_action_index)

            if (len(rewards) == n):
                # calculate n-step reward
                for rew in range(len(rewards)-1):
                    G += (gamma**rew)*rewards[rew]

                last_n_q = robot.get_last_n_q(actions_index[0])

                new_q = robot.get_current_q(new_action_index)

                # debug print
                if verbosity_level &gt;= 2: print(f&#34;  Q for action {new_action} with index {new_action_index} at voxel {robot.current_voxel}: {new_q}. Updating Q values.&#34;)

                # Update the Q-value for the current state and action pair
                last_n_q = last_n_q + alpha * (G+gamma**n*new_q - last_n_q)

                robot.set_last_n_q(actions_index[0], last_n_q)

                # only store the last n iterations
                del (rewards[0])
                del (states[0])
                del (actions[0])
                del (actions_index[0])

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;  Updated Qs for last Voxel:\n{robot.Q[robot.voxels_index_dict[robot.last_voxel]]}&#34;)

            # S &lt;- S&#39; &lt;- done in robot.do_move(); A &lt;- A&#39;
            current_action_index = new_action_index
            current_action = new_action

            # increase number of iterations in current episode
            i += 1
            # reset G
            G = 0

            # debug print every 10000 iterations
            if (i % 10000 == 0) and verbosity_level &gt;= 1:
                print(f&#34;    Current_iteration {i} in episode {episode}, current_pos {new_pos}, Out of bounds: {robot.out_of_bounds_counter}&#34;)
                print(f&#34;    Move / out of bound ratio: {robot.out_of_bounds_counter/i}, best reward: {best_reward}&#34;)
                print(f&#34;    alpha = {alpha} gamma = {gamma} epsilon = {epsilon}&#34;)

        # time calculation
        end_time = time.time()

        # concatenate episode to episode_length for return
        episode_lengths.append(i)

        # debug print
        if verbosity_level &gt;= 1:
            print(f&#34;    Episode {episode} ended with length {i}. Time for eposiode: {end_time-start_time} s.  Alpha = {alpha}                            &#34;,
                                       end=&#39;\r&#39;)

    # return algorithm name
    algo = &#39;sarsa_n_step&#39;

    # debug print
    if verbosity_level &gt;= 1:
        print(f&#34;&#34;)

    # return length of each episodes, algorithm name and last alpha
    return episode_lengths, algo, alpha

def learn(section_length, section, min_num_episodes, alpha, gamma, epsilon, queue=None, load=False,
          max_num_cycles=5, draw_robot=False, starting_pos=None, save_plot=True):
    &#34;&#34;&#34;
    Conduct learning on a robotic arm using the n-step SARSA algorithm and save the results.

    This function initializes a robot arm and performs learning cycles using the n-step SARSA algorithm.
    It supports loading pre-learned values, adjusting the starting position, and visualizing the robot&#39;s
    movements. The learning process continues until the robot arm successfully completes the task or
    reaches the maximum number of cycles. The learning results are saved to a file, and the learning
    progress can be plotted.

    Args:
      section_length (int): Length of each section of the robot arm.
      section (any): The specific section of the task to be learned.
      min_num_episodes (int): Minimum number of episodes for each learning cycle.
      alpha (float): The learning rate.
      gamma (float): The discount factor.
      epsilon (float): The exploration probability.
      queue (Queue, optional): Queue for inter-process communication. Default is None.
      load (bool): Whether to load pre-learned values. Default is False.
      max_num_cycles (int): Maximum number of learning cycles. Default is 5.
      draw_robot (bool): Flag to indicate if the robot should be visualized during the process. Default is False.
      starting_pos (list/tuple, optional): Starting position of the robot. Default is None.
      save_plot (bool): Flag to indicate if the learning plot should be saved. Default is True.

    Returns:
      list/tuple: The final angles of the robot arm after learning completion.
    &#34;&#34;&#34;
    # Learn section and save to file
    arm = bot.Robot_Arm(section_length=section_length, helix_section=section,
                        voxel_volume=2, num_axis=num_axis)

    if load is True:
        arm.load_learned_from_file()

    if starting_pos is not None:
        arm.set_starting_angles_rad(starting_pos)

    if queue is None and draw_robot is True:
        arm.show(draw_path=True, draw_voxels=True, zoom_path=True)

    # Learn until the Q values lead the arm into the finish
    episode_lengths = []
    for cycle in range(max_num_cycles):
        if queue is not None:
            queue.put((section, cycle, &#34;cycle&#34;))
            sarsa_verbosity_level = 0
        else:
            print(f&#34;section: {section}, cycle {cycle}&#34;)
            sarsa_verbosity_level = 1
        eps, _, alpha = n_step_sarsa(arm, min_num_episodes, alpha, gamma, epsilon,
                                     verbosity_level=sarsa_verbosity_level, queue=queue,
                                     section=section, episode_start=(cycle*min_num_episodes))
        episode_lengths = episode_lengths + eps
        finishing_angles_last_section = arm.get_finishing_angles_rad()
        if (finishing_angles_last_section[0] == &#34;Success&#34;) or (cycle == max_num_cycles-1):
            if queue is not None:
                queue.put((section, cycle, &#34;done&#34;))
            else:
                print(f&#34;    section: {section}, cycle {cycle}, DONE!&#34;)
            break

    arm.save_learned_to_file()

    if save_plot is True:
        fig, ax = plt.subplots(figsize=(10, 10))
        #ax.set_yscale(&#39;log&#39;)
        ax.plot(episode_lengths[0:150])

        ax.set_xlabel(&#39;Episodes&#39;)
        ax.set_ylabel(&#39;Episode length&#39;)
        plt.savefig(f&#39;learned_values_{num_axis}_axis/section_{section}_plot.png&#39;)

    if queue is None and len(episode_lengths) &gt; 0:
        print(f&#34;    average length of the last 100 episodes: {np.average(episode_lengths[-100:len(episode_lengths)])}&#34;)
        print(f&#34;    last 10 episode lengths: {episode_lengths[-10:len(episode_lengths)]}\n&#34;)

    if draw_robot is True:
        arm.animate_move_along_q_values(draw_path=True, draw_voxels=True, zoom_path=True)

    # Wait a moment so all queue data can be processed and process can return
    time.sleep(2)

    return finishing_angles_last_section[1]

def print_sections(num_sections):
    &#34;&#34;&#34;
    Print the header for sections in a formatted manner.

    This function prints a formatted header showing section numbers for a given number of sections.
    It is designed to visually organize output related to different sections in a structured format.

    Args:
      num_sections (int): The total number of sections to display in the header.

    Returns:
      None
    &#34;&#34;&#34;
    print(&#34;\n         &#34;, end=&#34;&#34;)
    for i in range(int(num_sections/2)-1):
        print(f&#34;     &#34;, end=&#34;&#34;)
    print(&#34;Sections&#34;)
    print(&#34;        &#34;, end=&#34;&#34;)
    for i in range(num_sections):
        print(f&#34; {(i+section_start):03d} &#34;, end=&#34;&#34;)
    print(&#34;\n========&#34;, end=&#34;&#34;)
    for i in range(num_sections):
        print(f&#34;=====&#34;, end=&#34;&#34;)
    print(&#34;\nCycle  |\nEpisode|&#34;, end=&#34;\r&#34;)

def monitor_queue(total_num_sections, queue):
    &#34;&#34;&#34;
    Monitor and display the progress of tasks using data from a queue.

    This function continuously monitors a queue for messages indicating the progress of various tasks.
    It updates the display to reflect the current status of each section in the process. The function
    handles different types of messages such as cycle updates, episode updates, and completion
    notifications. It&#39;s designed to provide a real-time progress overview in a multi-process or
    multi-threaded environment.

    Args:
      total_num_sections (int): Total number of sections in the task.
      queue (Queue): Queue object used for inter-process or inter-thread communication.

    Returns:
      None
    &#34;&#34;&#34;
    print_sections(total_num_sections)
    while True:
        if not queue.empty():
            result = queue.get()
            if result[2] == &#34;cycle&#34;:
                print(&#34;\033[A\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; {result[1]:03d}&#34;, end=&#34;\r&#34;)
                print(&#34;\033[B&#34;, end=&#34;&#34;)
            if result[2] == &#34;episode&#34;:
                print(&#34;\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; {result[1]:04d}&#34;, end=&#34;\r&#34;)
            if result[2] == &#34;done&#34;:
                print(&#34;\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; done&#34;, end=&#34;\r&#34;)
            if result[2] == &#34;next&#34;:
                iteration += 1
                print(f&#34;\n&#34;)
                print(f&#34;Iteration: {iteration}&#34;)
                print_sections(total_num_sections)
        time.sleep(0.05)

def learn_parallel(num_episodes, alpha, gamma, epsilon, num_processes=32, use_learned=False):
    &#34;&#34;&#34;
    Initiate parallel learning processes for a robotic task.

    This function sets up and initiates multiple parallel learning processes for a robotic task. Each process
    runs a learning algorithm on a different section of the task, with the possibility of utilizing previously
    learned values. The function manages process creation, execution, and monitoring. A queue is used for
    inter-process communication to monitor the progress of each section. The function ensures that the number
    of sections is a power of two and starts a separate monitoring thread to track and display the progress of
    the learning tasks.

    Note: This function assumes that &#39;learn&#39; and &#39;monitor_queue&#39; functions are defined and accessible.

    Args:
      num_episodes (int): Number of episodes for each learning process.
      alpha (float): Learning rate for the learning algorithm.
      gamma (float): Discount factor for the learning algorithm.
      epsilon (float): Exploration probability for the learning algorithm.
      num_processes (int, optional): Number of parallel processes to be created for learning. Default is 32.
      use_learned (bool, optional): Flag to indicate whether to use previously learned values. Default is False.

    Returns:
      None
    &#34;&#34;&#34;
    print(&#34;\nLearning in Parallel&#34;)

    processes = []

    queue = multiprocessing.Queue()

    total_sections = num_sections

    if not (learn_sections &gt; 0 and (learn_sections &amp; (learn_sections - 1)) == 0):
        print(f&#34;Number of parallel sections not a power of 2 ({learn_sections}). Aborting.&#34;)
        return

    # Start the queue monitoring thread
    monitor_thread = threading.Thread(target=monitor_queue, args=(learn_sections, queue,))
    monitor_thread.start()

    show_robot = False
    max_num_cycles=10

    section_length = 1/num_processes

    # Create and start processes
    num_processes = total_sections

    for i in range(section_start, section_start+learn_sections):
        args = (section_length, i, num_episodes, alpha, gamma, epsilon, queue, use_learned, max_num_cycles, show_robot, None, True)
        p = multiprocessing.Process(target=learn, args=args)
        p.start()
        processes.append(p)
        time.sleep(0.2)

    # Wait for all processes to finish
    all_processes_done = False
    while all_processes_done is False:
        all_processes_done = True
        delete_processes = []
        for p in processes:
            if not p.is_alive():
                delete_processes.append(p)
            else:
                all_processes_done = False
        processes = [p for p in processes if p not in delete_processes]

    # Kill monitoring thread
    monitor_thread.join()

def main():
    # Number of episodes per section
    num_episodes = 10000
    alpha = 0.1
    gamma = 0.99
    epsilon = 0.1

    starting_time = time.time()

    learn_parallel(num_episodes, alpha, gamma, epsilon, num_processes=num_sections, use_learned=True)

    print(&#34;\n\n    Parallel learning done, now going sequentially through each section and stitching them together.\n&#34;)

    num_episodes = 20
    alpha = 0.01
    finishing_angles = None

    for i in range(learn_sections):
        finishing_angles = learn(1/num_sections, i, num_episodes, alpha, gamma, epsilon, load=True,
                                 max_num_cycles=100, draw_robot=False, starting_pos=finishing_angles,
                                 save_plot=False)

    total_time = time.time()-starting_time

    arm = bot.Robot_Arm(section_length=1/num_sections, helix_section=0, voxel_volume=2, num_axis=num_axis)
    arm.load_learned_from_file()

    for i in range(learn_sections-1):
        arm.stitch_from_file()

    print(f&#34;MSE: {arm.calc_mse(support_points=1500)} (only correct when learning the whole helix)&#34;)
    arm.animate_move_along_q_values(draw_path=True, draw_voxels=True, zoom_path=True)
    arm.show(draw_path=True, draw_voxels=False, zoom_path=True, draw_q_path=True)
    print(f&#34;Total time: {total_time} seconds&#34;)

if __name__ == &#34;__main__&#34;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="reinforce.debug_pause"><code class="name flex">
<span>def <span class="ident">debug_pause</span></span>(<span>string_to_print=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Pause the program execution and optionally print a message.</p>
<p>This function pauses the execution of the program until the user presses Enter.
It can optionally print a message before pausing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>string_to_print</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>A string message to be printed before pausing. Default is None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def debug_pause(string_to_print=None):
    &#34;&#34;&#34;
    Pause the program execution and optionally print a message.

    This function pauses the execution of the program until the user presses Enter.
    It can optionally print a message before pausing.

    Args:
      string_to_print (str, optional): A string message to be printed before pausing. Default is None.

    Returns:
      None
    &#34;&#34;&#34;
    if string_to_print is not None:
        print(string_to_print)
    input(&#34;Press Enter to continue...&#34;)</code></pre>
</details>
</dd>
<dt id="reinforce.get_action_epsilon_greedy"><code class="name flex">
<span>def <span class="ident">get_action_epsilon_greedy</span></span>(<span>robot, epsilon: float = 0.1, verbosity_level=0) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Explore or exploit to choose an action for the robot based on epsilon-greedy strategy.</p>
<p>This function decides whether to explore a new action randomly or exploit the best known action
from the robot's Q-values. The choice between exploration and exploitation is made based on the
epsilon value provided. If exploring, a random action is chosen. If exploiting, the best action
based on current Q-values is selected.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>robot</code></strong> :&ensp;<code>Six_Axis_Robot_Arm object</code></dt>
<dd>The robot on which the action is to be performed.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The probability of choosing to explore rather than exploit. Default is 0.1.</dd>
<dt><strong><code>verbosity_level</code></strong> :&ensp;<code>int</code></dt>
<dd>Level of verbosity for output messages. Default is 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the action in the format of six float values and the action index.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_action_epsilon_greedy(robot, epsilon: float = 0.1, verbosity_level=0) -&gt; int:
    &#34;&#34;&#34;
    Explore or exploit to choose an action for the robot based on epsilon-greedy strategy.

    This function decides whether to explore a new action randomly or exploit the best known action
    from the robot&#39;s Q-values. The choice between exploration and exploitation is made based on the
    epsilon value provided. If exploring, a random action is chosen. If exploiting, the best action
    based on current Q-values is selected.

    Args:
      robot (Six_Axis_Robot_Arm object): The robot on which the action is to be performed.
      epsilon (float): The probability of choosing to explore rather than exploit. Default is 0.1.
      verbosity_level (int): Level of verbosity for output messages. Default is 0.

    Returns:
      tuple: A tuple containing the action in the format of six float values and the action index.
    &#34;&#34;&#34;
    if np.random.uniform(0, 1) &lt; epsilon:
        # Explore: select a random action
        action = robot.get_random_action()
        if verbosity_level &gt;= 2:
            print(f&#34;  Explore with action: {action}&#34;)
        return action
    else:
        # Exploit: select the best action based on current Q-values
        current_qs = robot.get_current_qs()
        action = np.argmax(current_qs)
        action_tuple = (robot.get_action_from_dict(action), action)
        if verbosity_level &gt;= 2:
            print(f&#34;  Exploiting with action: {action_tuple}&#34;)
        return action_tuple</code></pre>
</details>
</dd>
<dt id="reinforce.learn"><code class="name flex">
<span>def <span class="ident">learn</span></span>(<span>section_length, section, min_num_episodes, alpha, gamma, epsilon, queue=None, load=False, max_num_cycles=5, draw_robot=False, starting_pos=None, save_plot=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Conduct learning on a robotic arm using the n-step SARSA algorithm and save the results.</p>
<p>This function initializes a robot arm and performs learning cycles using the n-step SARSA algorithm.
It supports loading pre-learned values, adjusting the starting position, and visualizing the robot's
movements. The learning process continues until the robot arm successfully completes the task or
reaches the maximum number of cycles. The learning results are saved to a file, and the learning
progress can be plotted.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>section_length</code></strong> :&ensp;<code>int</code></dt>
<dd>Length of each section of the robot arm.</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>any</code></dt>
<dd>The specific section of the task to be learned.</dd>
<dt><strong><code>min_num_episodes</code></strong> :&ensp;<code>int</code></dt>
<dd>Minimum number of episodes for each learning cycle.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>The learning rate.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The discount factor.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The exploration probability.</dd>
<dt><strong><code>queue</code></strong> :&ensp;<code>Queue</code>, optional</dt>
<dd>Queue for inter-process communication. Default is None.</dd>
<dt><strong><code>load</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to load pre-learned values. Default is False.</dd>
<dt><strong><code>max_num_cycles</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of learning cycles. Default is 5.</dd>
<dt><strong><code>draw_robot</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to indicate if the robot should be visualized during the process. Default is False.</dd>
<dt>starting_pos (list/tuple, optional): Starting position of the robot. Default is None.</dt>
<dt><strong><code>save_plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to indicate if the learning plot should be saved. Default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>list/tuple: The final angles of the robot arm after learning completion.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn(section_length, section, min_num_episodes, alpha, gamma, epsilon, queue=None, load=False,
          max_num_cycles=5, draw_robot=False, starting_pos=None, save_plot=True):
    &#34;&#34;&#34;
    Conduct learning on a robotic arm using the n-step SARSA algorithm and save the results.

    This function initializes a robot arm and performs learning cycles using the n-step SARSA algorithm.
    It supports loading pre-learned values, adjusting the starting position, and visualizing the robot&#39;s
    movements. The learning process continues until the robot arm successfully completes the task or
    reaches the maximum number of cycles. The learning results are saved to a file, and the learning
    progress can be plotted.

    Args:
      section_length (int): Length of each section of the robot arm.
      section (any): The specific section of the task to be learned.
      min_num_episodes (int): Minimum number of episodes for each learning cycle.
      alpha (float): The learning rate.
      gamma (float): The discount factor.
      epsilon (float): The exploration probability.
      queue (Queue, optional): Queue for inter-process communication. Default is None.
      load (bool): Whether to load pre-learned values. Default is False.
      max_num_cycles (int): Maximum number of learning cycles. Default is 5.
      draw_robot (bool): Flag to indicate if the robot should be visualized during the process. Default is False.
      starting_pos (list/tuple, optional): Starting position of the robot. Default is None.
      save_plot (bool): Flag to indicate if the learning plot should be saved. Default is True.

    Returns:
      list/tuple: The final angles of the robot arm after learning completion.
    &#34;&#34;&#34;
    # Learn section and save to file
    arm = bot.Robot_Arm(section_length=section_length, helix_section=section,
                        voxel_volume=2, num_axis=num_axis)

    if load is True:
        arm.load_learned_from_file()

    if starting_pos is not None:
        arm.set_starting_angles_rad(starting_pos)

    if queue is None and draw_robot is True:
        arm.show(draw_path=True, draw_voxels=True, zoom_path=True)

    # Learn until the Q values lead the arm into the finish
    episode_lengths = []
    for cycle in range(max_num_cycles):
        if queue is not None:
            queue.put((section, cycle, &#34;cycle&#34;))
            sarsa_verbosity_level = 0
        else:
            print(f&#34;section: {section}, cycle {cycle}&#34;)
            sarsa_verbosity_level = 1
        eps, _, alpha = n_step_sarsa(arm, min_num_episodes, alpha, gamma, epsilon,
                                     verbosity_level=sarsa_verbosity_level, queue=queue,
                                     section=section, episode_start=(cycle*min_num_episodes))
        episode_lengths = episode_lengths + eps
        finishing_angles_last_section = arm.get_finishing_angles_rad()
        if (finishing_angles_last_section[0] == &#34;Success&#34;) or (cycle == max_num_cycles-1):
            if queue is not None:
                queue.put((section, cycle, &#34;done&#34;))
            else:
                print(f&#34;    section: {section}, cycle {cycle}, DONE!&#34;)
            break

    arm.save_learned_to_file()

    if save_plot is True:
        fig, ax = plt.subplots(figsize=(10, 10))
        #ax.set_yscale(&#39;log&#39;)
        ax.plot(episode_lengths[0:150])

        ax.set_xlabel(&#39;Episodes&#39;)
        ax.set_ylabel(&#39;Episode length&#39;)
        plt.savefig(f&#39;learned_values_{num_axis}_axis/section_{section}_plot.png&#39;)

    if queue is None and len(episode_lengths) &gt; 0:
        print(f&#34;    average length of the last 100 episodes: {np.average(episode_lengths[-100:len(episode_lengths)])}&#34;)
        print(f&#34;    last 10 episode lengths: {episode_lengths[-10:len(episode_lengths)]}\n&#34;)

    if draw_robot is True:
        arm.animate_move_along_q_values(draw_path=True, draw_voxels=True, zoom_path=True)

    # Wait a moment so all queue data can be processed and process can return
    time.sleep(2)

    return finishing_angles_last_section[1]</code></pre>
</details>
</dd>
<dt id="reinforce.learn_parallel"><code class="name flex">
<span>def <span class="ident">learn_parallel</span></span>(<span>num_episodes, alpha, gamma, epsilon, num_processes=32, use_learned=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Initiate parallel learning processes for a robotic task.</p>
<p>This function sets up and initiates multiple parallel learning processes for a robotic task. Each process
runs a learning algorithm on a different section of the task, with the possibility of utilizing previously
learned values. The function manages process creation, execution, and monitoring. A queue is used for
inter-process communication to monitor the progress of each section. The function ensures that the number
of sections is a power of two and starts a separate monitoring thread to track and display the progress of
the learning tasks.</p>
<p>Note: This function assumes that 'learn' and 'monitor_queue' functions are defined and accessible.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_episodes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of episodes for each learning process.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Learning rate for the learning algorithm.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Discount factor for the learning algorithm.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Exploration probability for the learning algorithm.</dd>
<dt><strong><code>num_processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to be created for learning. Default is 32.</dd>
<dt><strong><code>use_learned</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag to indicate whether to use previously learned values. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_parallel(num_episodes, alpha, gamma, epsilon, num_processes=32, use_learned=False):
    &#34;&#34;&#34;
    Initiate parallel learning processes for a robotic task.

    This function sets up and initiates multiple parallel learning processes for a robotic task. Each process
    runs a learning algorithm on a different section of the task, with the possibility of utilizing previously
    learned values. The function manages process creation, execution, and monitoring. A queue is used for
    inter-process communication to monitor the progress of each section. The function ensures that the number
    of sections is a power of two and starts a separate monitoring thread to track and display the progress of
    the learning tasks.

    Note: This function assumes that &#39;learn&#39; and &#39;monitor_queue&#39; functions are defined and accessible.

    Args:
      num_episodes (int): Number of episodes for each learning process.
      alpha (float): Learning rate for the learning algorithm.
      gamma (float): Discount factor for the learning algorithm.
      epsilon (float): Exploration probability for the learning algorithm.
      num_processes (int, optional): Number of parallel processes to be created for learning. Default is 32.
      use_learned (bool, optional): Flag to indicate whether to use previously learned values. Default is False.

    Returns:
      None
    &#34;&#34;&#34;
    print(&#34;\nLearning in Parallel&#34;)

    processes = []

    queue = multiprocessing.Queue()

    total_sections = num_sections

    if not (learn_sections &gt; 0 and (learn_sections &amp; (learn_sections - 1)) == 0):
        print(f&#34;Number of parallel sections not a power of 2 ({learn_sections}). Aborting.&#34;)
        return

    # Start the queue monitoring thread
    monitor_thread = threading.Thread(target=monitor_queue, args=(learn_sections, queue,))
    monitor_thread.start()

    show_robot = False
    max_num_cycles=10

    section_length = 1/num_processes

    # Create and start processes
    num_processes = total_sections

    for i in range(section_start, section_start+learn_sections):
        args = (section_length, i, num_episodes, alpha, gamma, epsilon, queue, use_learned, max_num_cycles, show_robot, None, True)
        p = multiprocessing.Process(target=learn, args=args)
        p.start()
        processes.append(p)
        time.sleep(0.2)

    # Wait for all processes to finish
    all_processes_done = False
    while all_processes_done is False:
        all_processes_done = True
        delete_processes = []
        for p in processes:
            if not p.is_alive():
                delete_processes.append(p)
            else:
                all_processes_done = False
        processes = [p for p in processes if p not in delete_processes]

    # Kill monitoring thread
    monitor_thread.join()</code></pre>
</details>
</dd>
<dt id="reinforce.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    # Number of episodes per section
    num_episodes = 10000
    alpha = 0.1
    gamma = 0.99
    epsilon = 0.1

    starting_time = time.time()

    learn_parallel(num_episodes, alpha, gamma, epsilon, num_processes=num_sections, use_learned=True)

    print(&#34;\n\n    Parallel learning done, now going sequentially through each section and stitching them together.\n&#34;)

    num_episodes = 20
    alpha = 0.01
    finishing_angles = None

    for i in range(learn_sections):
        finishing_angles = learn(1/num_sections, i, num_episodes, alpha, gamma, epsilon, load=True,
                                 max_num_cycles=100, draw_robot=False, starting_pos=finishing_angles,
                                 save_plot=False)

    total_time = time.time()-starting_time

    arm = bot.Robot_Arm(section_length=1/num_sections, helix_section=0, voxel_volume=2, num_axis=num_axis)
    arm.load_learned_from_file()

    for i in range(learn_sections-1):
        arm.stitch_from_file()

    print(f&#34;MSE: {arm.calc_mse(support_points=1500)} (only correct when learning the whole helix)&#34;)
    arm.animate_move_along_q_values(draw_path=True, draw_voxels=True, zoom_path=True)
    arm.show(draw_path=True, draw_voxels=False, zoom_path=True, draw_q_path=True)
    print(f&#34;Total time: {total_time} seconds&#34;)</code></pre>
</details>
</dd>
<dt id="reinforce.monitor_queue"><code class="name flex">
<span>def <span class="ident">monitor_queue</span></span>(<span>total_num_sections, queue)</span>
</code></dt>
<dd>
<div class="desc"><p>Monitor and display the progress of tasks using data from a queue.</p>
<p>This function continuously monitors a queue for messages indicating the progress of various tasks.
It updates the display to reflect the current status of each section in the process. The function
handles different types of messages such as cycle updates, episode updates, and completion
notifications. It's designed to provide a real-time progress overview in a multi-process or
multi-threaded environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>total_num_sections</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of sections in the task.</dd>
<dt><strong><code>queue</code></strong> :&ensp;<code>Queue</code></dt>
<dd>Queue object used for inter-process or inter-thread communication.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monitor_queue(total_num_sections, queue):
    &#34;&#34;&#34;
    Monitor and display the progress of tasks using data from a queue.

    This function continuously monitors a queue for messages indicating the progress of various tasks.
    It updates the display to reflect the current status of each section in the process. The function
    handles different types of messages such as cycle updates, episode updates, and completion
    notifications. It&#39;s designed to provide a real-time progress overview in a multi-process or
    multi-threaded environment.

    Args:
      total_num_sections (int): Total number of sections in the task.
      queue (Queue): Queue object used for inter-process or inter-thread communication.

    Returns:
      None
    &#34;&#34;&#34;
    print_sections(total_num_sections)
    while True:
        if not queue.empty():
            result = queue.get()
            if result[2] == &#34;cycle&#34;:
                print(&#34;\033[A\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; {result[1]:03d}&#34;, end=&#34;\r&#34;)
                print(&#34;\033[B&#34;, end=&#34;&#34;)
            if result[2] == &#34;episode&#34;:
                print(&#34;\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; {result[1]:04d}&#34;, end=&#34;\r&#34;)
            if result[2] == &#34;done&#34;:
                print(&#34;\033[C\033[C\033[C\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                for i in range(result[0]-section_start):
                    print(&#34;\033[C\033[C\033[C\033[C\033[C&#34;, end=&#34;&#34;)
                print(f&#34; done&#34;, end=&#34;\r&#34;)
            if result[2] == &#34;next&#34;:
                iteration += 1
                print(f&#34;\n&#34;)
                print(f&#34;Iteration: {iteration}&#34;)
                print_sections(total_num_sections)
        time.sleep(0.05)</code></pre>
</details>
</dd>
<dt id="reinforce.n_step_sarsa"><code class="name flex">
<span>def <span class="ident">n_step_sarsa</span></span>(<span>robot, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1, verbosity_level=0, queue=None, section=None, episode_start=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform the n-step SARSA algorithm on a given robot for a specified number of episodes.</p>
<p>This function implements the n-step SARSA learning algorithm. It iteratively updates the Q-values
based on the observed rewards and the chosen actions using an epsilon-greedy policy. The function
also allows for dynamic adjustment of the learning rate alpha and supports message passing through
a queue for parallel execution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>robot</code></strong> :&ensp;<code>Six_Axis_Robot_Arm object</code></dt>
<dd>The robot object to apply the n-step SARSA algorithm on.</dd>
<dt><strong><code>num_episodes</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of episodes to run the algorithm.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>The learning rate. Default is 0.1.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The discount factor. Default is 0.99.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>The exploration probability. Default is 0.1.</dd>
<dt><strong><code>verbosity_level</code></strong> :&ensp;<code>int</code></dt>
<dd>Level of verbosity for output messages. Default is 0.</dd>
<dt><strong><code>queue</code></strong> :&ensp;<code>Queue</code>, optional</dt>
<dd>A queue object for inter-process communication. Default is None.</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>any</code></dt>
<dd>The section or segment of the task being processed. Default is None.</dd>
<dt><strong><code>episode_start</code></strong> :&ensp;<code>int</code></dt>
<dd>The starting episode number. Default is 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing episode lengths, algorithm name, and the final alpha value.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def n_step_sarsa(robot, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1, verbosity_level=0,
                 queue=None, section=None, episode_start=0):
    &#34;&#34;&#34;
    Perform the n-step SARSA algorithm on a given robot for a specified number of episodes.

    This function implements the n-step SARSA learning algorithm. It iteratively updates the Q-values
    based on the observed rewards and the chosen actions using an epsilon-greedy policy. The function
    also allows for dynamic adjustment of the learning rate alpha and supports message passing through
    a queue for parallel execution.

    Args:
      robot (Six_Axis_Robot_Arm object): The robot object to apply the n-step SARSA algorithm on.
      num_episodes (int): The number of episodes to run the algorithm.
      alpha (float): The learning rate. Default is 0.1.
      gamma (float): The discount factor. Default is 0.99.
      epsilon (float): The exploration probability. Default is 0.1.
      verbosity_level (int): Level of verbosity for output messages. Default is 0.
      queue (Queue, optional): A queue object for inter-process communication. Default is None.
      section (any): The section or segment of the task being processed. Default is None.
      episode_start (int): The starting episode number. Default is 0.

    Returns:
      tuple: A tuple containing episode lengths, algorithm name, and the final alpha value.
    &#34;&#34;&#34;
    # n step sarsa, n = 5
    n = 5
    # list to save each episodes length
    episode_lengths = []
    # when queue is given, only write into queue every 4 seconds as to not overwhelm the queue
    # last queue update saves the time of the last update of the queue
    last_queue_update = 0

    # do num_episode episodes
    for episode in range(num_episodes):

        # decrease alpha every 100 episodes by a factor of 0.98 and as long as alpha is &gt; 0.001
        if (alpha &gt; 0.001) and ((episode+episode_start) % 100 == 0) and ((episode+episode_start) != 0):
            alpha *= 0.98

        # time measurement
        start_time = time.time()

        # Initialize the starting state S (choosing from the starting positions)
        robot.reset()

        # debug print
        if verbosity_level &gt;= 2:
            print(f&#34;Robot initialized, starting SARSA, Starting angles: {robot.get_joint_angles_rad()}, Starting position: {robot.rob.end_effector_position()} Choosing action!&#34;)
            print(f&#34;   Initial Qs for starting_voxel:\n{robot.Q[robot.voxels_index_dict[(-500, 0, 0)]]}&#34;)

        # current state contains the current TCP of the robot
        current_state = robot.get_tcp()

        # Choose the first action A from S using Q (epsilon-greedy)
        current_action, current_action_index = get_action_epsilon_greedy(robot, epsilon, verbosity_level=verbosity_level)
        if verbosity_level &gt;= 2:
            print(&#39;current_action: &#39;, current_action)
            print(&#39;current_action_index: &#39;, current_action_index)

        # number of iterations in current episode
        i = 0

        # lists to save last n values
        states = []
        actions = []
        actions_index = []
        rewards = []

        # save first state
        states.append(current_state)
        actions.append(current_action)
        actions_index.append(current_action_index)

        # weightet sum of last rewards
        G = 0

        # initialize best reward for debug printing
        best_reward = -100

        # Send debug information into queue
        if queue is not None:
            if time.time() - last_queue_update &gt; 4:
                queue.put((section, episode, &#34;episode&#34;))
                last_queue_update = time.time()

        # Loop for each step of the episode
        done = False

        # Until all episodes are done
        while not done:

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;\nNew loop, iteration = {i}, current Voxel: {robot.current_voxel}&#34;)
                debug_pause(f&#34; Qs for current Voxel:\n {robot.Q[robot.voxels_index_dict[robot.current_voxel]]}&#34;)

            # Save Q
            last_q = robot.get_current_q(actions_index[0])
            if verbosity_level &gt;= 2:
                print(f&#34;  Q for current action {current_action} with index {current_action_index} at voxel {robot.current_voxel}: {last_q}. Doing SAR&#34;)

            # Take action A, observe R, S&#39;
            new_pos, reward, done = robot.do_move(current_action_index)

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;  New position: {new_pos}, reward: {reward}, new Voxel: {robot.current_voxel}. Doing SA&#34;)
                print(f&#34;  Qs for new position:\n{robot.Q[robot.voxels_index_dict[robot.current_voxel]]}&#34;)

            if reward &gt; best_reward:
                best_reward = reward

            # Choose A&#39; from S&#39; using Q (epsilon-greedy)
            new_action, new_action_index = get_action_epsilon_greedy(robot, epsilon, verbosity_level=verbosity_level)

            # Append states and rewards to lists
            rewards.append(reward)
            states.append(new_pos)
            actions.append(new_action)
            actions_index.append(new_action_index)

            if (len(rewards) == n):
                # calculate n-step reward
                for rew in range(len(rewards)-1):
                    G += (gamma**rew)*rewards[rew]

                last_n_q = robot.get_last_n_q(actions_index[0])

                new_q = robot.get_current_q(new_action_index)

                # debug print
                if verbosity_level &gt;= 2: print(f&#34;  Q for action {new_action} with index {new_action_index} at voxel {robot.current_voxel}: {new_q}. Updating Q values.&#34;)

                # Update the Q-value for the current state and action pair
                last_n_q = last_n_q + alpha * (G+gamma**n*new_q - last_n_q)

                robot.set_last_n_q(actions_index[0], last_n_q)

                # only store the last n iterations
                del (rewards[0])
                del (states[0])
                del (actions[0])
                del (actions_index[0])

            # debug print
            if verbosity_level &gt;= 2:
                print(f&#34;  Updated Qs for last Voxel:\n{robot.Q[robot.voxels_index_dict[robot.last_voxel]]}&#34;)

            # S &lt;- S&#39; &lt;- done in robot.do_move(); A &lt;- A&#39;
            current_action_index = new_action_index
            current_action = new_action

            # increase number of iterations in current episode
            i += 1
            # reset G
            G = 0

            # debug print every 10000 iterations
            if (i % 10000 == 0) and verbosity_level &gt;= 1:
                print(f&#34;    Current_iteration {i} in episode {episode}, current_pos {new_pos}, Out of bounds: {robot.out_of_bounds_counter}&#34;)
                print(f&#34;    Move / out of bound ratio: {robot.out_of_bounds_counter/i}, best reward: {best_reward}&#34;)
                print(f&#34;    alpha = {alpha} gamma = {gamma} epsilon = {epsilon}&#34;)

        # time calculation
        end_time = time.time()

        # concatenate episode to episode_length for return
        episode_lengths.append(i)

        # debug print
        if verbosity_level &gt;= 1:
            print(f&#34;    Episode {episode} ended with length {i}. Time for eposiode: {end_time-start_time} s.  Alpha = {alpha}                            &#34;,
                                       end=&#39;\r&#39;)

    # return algorithm name
    algo = &#39;sarsa_n_step&#39;

    # debug print
    if verbosity_level &gt;= 1:
        print(f&#34;&#34;)

    # return length of each episodes, algorithm name and last alpha
    return episode_lengths, algo, alpha</code></pre>
</details>
</dd>
<dt id="reinforce.print_sections"><code class="name flex">
<span>def <span class="ident">print_sections</span></span>(<span>num_sections)</span>
</code></dt>
<dd>
<div class="desc"><p>Print the header for sections in a formatted manner.</p>
<p>This function prints a formatted header showing section numbers for a given number of sections.
It is designed to visually organize output related to different sections in a structured format.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_sections</code></strong> :&ensp;<code>int</code></dt>
<dd>The total number of sections to display in the header.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_sections(num_sections):
    &#34;&#34;&#34;
    Print the header for sections in a formatted manner.

    This function prints a formatted header showing section numbers for a given number of sections.
    It is designed to visually organize output related to different sections in a structured format.

    Args:
      num_sections (int): The total number of sections to display in the header.

    Returns:
      None
    &#34;&#34;&#34;
    print(&#34;\n         &#34;, end=&#34;&#34;)
    for i in range(int(num_sections/2)-1):
        print(f&#34;     &#34;, end=&#34;&#34;)
    print(&#34;Sections&#34;)
    print(&#34;        &#34;, end=&#34;&#34;)
    for i in range(num_sections):
        print(f&#34; {(i+section_start):03d} &#34;, end=&#34;&#34;)
    print(&#34;\n========&#34;, end=&#34;&#34;)
    for i in range(num_sections):
        print(f&#34;=====&#34;, end=&#34;&#34;)
    print(&#34;\nCycle  |\nEpisode|&#34;, end=&#34;\r&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="reinforce.debug_pause" href="#reinforce.debug_pause">debug_pause</a></code></li>
<li><code><a title="reinforce.get_action_epsilon_greedy" href="#reinforce.get_action_epsilon_greedy">get_action_epsilon_greedy</a></code></li>
<li><code><a title="reinforce.learn" href="#reinforce.learn">learn</a></code></li>
<li><code><a title="reinforce.learn_parallel" href="#reinforce.learn_parallel">learn_parallel</a></code></li>
<li><code><a title="reinforce.main" href="#reinforce.main">main</a></code></li>
<li><code><a title="reinforce.monitor_queue" href="#reinforce.monitor_queue">monitor_queue</a></code></li>
<li><code><a title="reinforce.n_step_sarsa" href="#reinforce.n_step_sarsa">n_step_sarsa</a></code></li>
<li><code><a title="reinforce.print_sections" href="#reinforce.print_sections">print_sections</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>